{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA FORMAT PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "paths = os.listdir('datasets/train')\n",
    "labels = [random.choice([0,1]) for x in range(len(paths))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([paths,labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()\n",
    "df = df.rename(columns={0:\"paths\",1:\"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataformat(path,name):\n",
    "    paths = os.listdir(path)\n",
    "    labels = [random.choice([0,1]) for x in range(len(paths))]\n",
    "    df = pd.DataFrame([paths,labels])\n",
    "    df = df.transpose()\n",
    "    df = df.rename(columns={0:\"paths\",1:\"labels\"})\n",
    "    df.to_csv(f'datasets/{name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataformat('datasets/test',name = 'test')\n",
    "dataformat('datasets/valid',name = 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOM DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,cv2\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import pandas as pd\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_csv,absolute_path=\"datasets/train\"):\n",
    "        self.path_csv = path_csv\n",
    "        self.data = pd.read_csv(path_csv)\n",
    "        self.absolute_path = absolute_path\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data['paths'][idx]\n",
    "        label = self.data['labels'][idx]\n",
    "        image = cv2.imread(self.absolute_path+\"/\"+image_path)\n",
    "        image = cv2.resize(image,(224,224))\n",
    "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = CustomDataset('datasets/train.csv')\n",
    "data_test = CustomDataset('datasets/test.csv')\n",
    "BATCH_SIZE = 32\n",
    "TRAINLOADER = DataLoader(data_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "TESTLOADER = DataLoader(data_test, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inputs, labels in TRAINLOADER:\n",
    "#     print(inputs.shape)\n",
    "#     print(labels.shape)\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_features = (224,224,3)\n",
    "class Custom_Model(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Custom_Model, self).__init__()\n",
    "        self.conv3x3 = nn.Conv2d(in_channels=224,out_channels=112,kernel_size=3)\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=112,out_channels=64,kernel_size=1)\n",
    "        self.linear = nn.Linear(in_features= 64*222, out_features= num_classes)\n",
    "    def forward(self,x):\n",
    "        b = x.shape[0]\n",
    "        x = self.conv3x3(x)\n",
    "        x = self.conv1x1(x)\n",
    "        x = x.view(b,-1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = torch.rand((8,224,224,3))\n",
    "model = Custom_Model()\n",
    "output_test = model(input_test)\n",
    "output_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        log_probs = torch.log_softmax(input, dim=1)\n",
    "        loss = -log_probs.gather(dim=1, index=target.view(-1, 1))\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2939)\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(3, 5)  # Example input tensor with batch size 3 and 5 classes\n",
    "target_tensor = torch.tensor([1, 0, 3])  # Example target tensor with class indices\n",
    "\n",
    "# Instantiate the custom loss\n",
    "custom_loss = CustomCrossEntropyLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = custom_loss(input_tensor, target_tensor)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Custom_Model()\n",
    "criterion = CustomCrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "EPOCHS = 10\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=float(lr))\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.99it/s, loss=530] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.09it/s, loss=120]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.43it/s, loss=1.59e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.41it/s, loss=3.34e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.35it/s, loss=2.6e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.43it/s, loss=738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.41it/s, loss=149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.30it/s, loss=1.72e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.33it/s, loss=122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  6.92it/s, loss=361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(enumerate(TRAINLOADER), total=len(TRAINLOADER), leave=True)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in loop:\n",
    "        inputs = inputs.to(device).to(torch.float32)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "    print(\"Checkpoint saved for epoch:\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ckpt continues training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint.pth')\n",
    "# Load the model state dict\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Load the optimizer state dict\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# Load the loss\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.83it/s, loss=361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.73it/s, loss=361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.85it/s, loss=361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.66it/s, loss=361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.49it/s, loss=361]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved for epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "additional_epochs = 5\n",
    "start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch after the last checkpoint\n",
    "\n",
    "# Continue training\n",
    "for epoch in range(start_epoch,start_epoch+additional_epochs):\n",
    "    loop = tqdm(enumerate(TRAINLOADER), total=len(TRAINLOADER), leave=True)\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in loop:\n",
    "        inputs = inputs.to(device).to(torch.float32)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(inputs)\n",
    "        # loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }\n",
    "    torch.save(checkpoint, 'checkpoint.pth')\n",
    "    print(\"Checkpoint saved for epoch:\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0061, 0.0617]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('model.pth')\n",
    "input = torch.rand(1,224,224,3)\n",
    "out = model(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
